{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aram Dovlatyan - Assignment 1 for PyTorch Deep Learning Course\n",
    "\n",
    "### Exploring the fundamentals of Linear Alegbra with PyTorch\n",
    "\n",
    "An short introduction about PyTorch and about the chosen functions that relate to Linear Algebra. \n",
    "- torch.det\n",
    "- torch.lstsq\n",
    "- torch.eig\n",
    "- torch.qr\n",
    "- torch.svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import torch and other required modules\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1 - torch.det\n",
    "\n",
    "The function torch.det calculates the determinant of a *square* matrix or groups of *square* matrices. It takes a tensor as an input and without loss of too much generality we can think of a tensor as a batch a multiple matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 7.],\n",
      "        [2., 3.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-2.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "A = torch.tensor([[4.,7.], [2.,3.]]) # dimensions must be nxn, 3 parameters for input\n",
    "print(A)\n",
    "\n",
    "torch.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a square matrix with dimensions 2x2 ahead of time, then call and apply the .det (determinant) function on the square matrix. The determinant function returns a value of -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3742,  2.0282,  1.9084],\n",
      "         [ 0.6228,  0.1879, -1.8463],\n",
      "         [ 0.7574, -3.7132, -0.6964]],\n",
      "\n",
      "        [[-1.0723, -0.7526, -0.6909],\n",
      "         [-0.5210, -1.1502,  0.1008],\n",
      "         [ 1.6173,  0.3579, -0.1398]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-4.0272, -1.3580])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 2-  working\n",
    "\n",
    "B = torch.randn(2,3,3)\n",
    "print(B)\n",
    "\n",
    "B.det()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a tensor that consists of 2 matrices, each matrix is a 3x3 square matrix. Our matrix consists of random input and we apply the determinant function on the tensor. It calculates a determinant for each 3x3 matrix and lists it in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a floating point tensor as input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b42f314ce37c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 -\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected a floating point tensor as input"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking\n",
    "C = torch.tensor([[4,7], [2,3]])\n",
    "torch.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we created a tensor that consists of 2 matrices that are each 2x2, however, we used integers as input and didn't specify the datatype explicitly. Therefore, PyTorch assumed the datatype to be int64 or in other words, integer. It seems that the determinant function only works on tensors with the floating-point datatype. In order to fix it, we have to turn our Tensor \"C\" into a floating-point tensor and that can be done by adding a .0 decimal point to any number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the torch.det() function\n",
    "\n",
    "As we know in Linear Algebra, the determinant is a fundamental characteristic for describing matrices. The determinant obeys many important identities. They are used to construct adjoint matrices, solve a system of n linear equations in n variables by utilizing Cramer's rule, applications in analytic geometry to see how area in a matrix scales from a linear transformation as well as determine volume for outputs from linear transformations.\n",
    "\n",
    "In general, you would use this function to calculate determinants and build on it for computational efficiency if necessary. Determinants often have multiple different mathematical definitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2 - torch.lstsq\n",
    "\n",
    "This function computes the solution to the least square and least norm problems for a full-rank matrix A of size (m x n ) and a matrix B of size (m x k). This is fundamental for fitting linear regression models to data. Ordinary Least Squares is an estimation technique that can be represented with linear algebra or calculus.\n",
    "\n",
    "The returned tensor X, contains solution to each column(variable) in first $n$ rows, the remaining $m-n$ contains residuals for the model in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0],\n",
      "        [2, 1],\n",
      "        [3, 3]])\n",
      "\n",
      " The solution of the best fit where the first 2 rows contain the solution and the final row is the residual.\n",
      "tensor([[-1.6667],\n",
      "        [ 1.5000],\n",
      "        [ 0.4082]])\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "points = torch.tensor([[1,0], [2,1], [3,3]]) # Let these 3 vectors be 3 points in the cartesian coordinate plane.\n",
    "print(points)\n",
    "\n",
    "A = torch.tensor([[1.0,1], [1,2], [1,3]]) # create matrix A for inputs(X coordinates) that satifies Ax = B\n",
    "B = torch.tensor([0.0,1,3]) # create matrix B for outputs(Y Coordinates) that satisfies Ax=B\n",
    "\n",
    "\n",
    "print(\"\\n The solution of the best fit where the first 2 rows contain the solution and the final row is the residual.\")\n",
    "X, _ = torch.lstsq(B,A) # determining the values of the X matrix which minimizes the error |Ax-B| and approximates the equality\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we take 3 points and represent them with 2 tensors. The first tensor A, represents the X-coordinates of these ordered points, the second tensor B, represents the corresponding Y-coordinates for the points. Fitting a system of linear equations for the best approximation means satisfying the equation $Ax=B$ to the best of ability, in other words, minimizing $(Ax-B)^2$ or the squared error in the model. \n",
    "\n",
    "We have determined a linear equation that takes the form of a constant/intercept and variable, X. It looks like <b>Y= B<sub>o</sub> + B<sub>1</sub>*x</b>. The first row in our solution tensor(X) is the intercept, and the 2nd row is the coefficient for the x variable.\n",
    "\n",
    "The least squares regression line is $y = 3/2x - 5/3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4.5000e+00],\n",
      "        [ 8.0000e-02],\n",
      "        [-1.1296e-10],\n",
      "        [-2.0136e-07],\n",
      "        [ 2.5062e-08]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "# Fit a quadratic polynomial of the form y = Co + C1x + C2x^2\n",
    "\n",
    "A = torch.tensor([[1.0,0,0], [1,5,25,], [1,10,100], [1,15,225], [1,20,400]]) # data for every 5 years of sample population\n",
    "B = torch.tensor(([4.5,4.9,5.3,5.7,6.1])) # data for populations in millions per 5 year cross-section\n",
    "\n",
    "X, _ = torch.lstsq(B,A)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create a new data table replicating a population for some hypothetical species that is measured every 5 years. The population is in the B tensor denoted by millions, (ie 4.5 million in the first year). We have fit a least squares model to the data and returned a solution with 4.5 as our intercept, 8.0 as coefficient for the first column, and $-1.12*10^{-10}$ for the 2nd column, our quadratic term, however this value is approximately 0, therefore we will exclude it as it wont make an impact on our equation performance.\n",
    "\n",
    "We fit a regression quadratic polynomial of the form <b>y = Co + C<sub>1</sub>(x) + C<sub>2</sub>(x^2)</b>. Our final linear polynomial looks like this, \n",
    "$y = 4.5 + 0.08x$ where y is our population and x is the number of years, for example, if we input 5 for x(after the first 5 years), we would expect a return of the population closs to 4.9.\n",
    "\n",
    "Indeed $y(5) = 4.5 + 0.08(5) = 4.5 + 0.40 = 4.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected A and b to have same size at dim 0, but A has 4 rows and B has 3 rows",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-33841c767161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected A and b to have same size at dim 0, but A has 4 rows and B has 3 rows"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking\n",
    "\n",
    "A = torch.tensor([[1.0, 2, 5], [1, 4, 10,], [1, 5, 12.5], [1, 7, 16.5]])\n",
    "B = torch.tensor(([5.5, 7, 9.5]))\n",
    "\n",
    "X, _ = torch.lstsq(B,A)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tensor A contains 4 rows of data but our tensor B has 3. This means that we are missing an output, we can't fit a regression function without an even number of inputs and outputs in the data. When you use the least squares function, make sure you have the correct number of dimensions setup throughout input and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the torch.lstsq() function\n",
    "\n",
    "This function is helpful for computing solutions to least squares fitting of a system of linear equations. Generally you can use this function to fit linear equations that also capture non-linearity with quadratic terms, by our definition of linear, we mean that our input parameters are linear. Least squares as an estimator has applications across modeling *inconsistent* systems of linear equations and approximating functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3 - torch.eig\n",
    "\n",
    "This function computes the eigenvalues and eigenvectors of *real* square matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.eig(\n",
      "eigenvalues=tensor([[ 2.,  0.],\n",
      "        [-1.,  0.]]),\n",
      "eigenvectors=tensor([[1., 0.],\n",
      "        [0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "A = torch.tensor([[2.0,0], [0,-1]])\n",
    "\n",
    "X =torch.eig(A, eigenvectors = True)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we computed the eigenvalues and eigenvectors of a 2x2 square matrix. Our respective *real* eigenvalues are the first element in each row of the returned eigenvalue tensor (the first output), so 2 and -1 are the eigenvalues. The eigenvectors in the second output are represented by each row, so (1,0) and (0,1) are the eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.eig(\n",
      "eigenvalues=tensor([[3., 0.],\n",
      "        [2., 0.],\n",
      "        [1., 0.]]),\n",
      "eigenvectors=tensor([[0.0000, 0.0000, 0.5774],\n",
      "        [0.4472, 0.7071, 0.5774],\n",
      "        [0.8944, 0.7071, 0.5774]]))\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "\n",
    "A = torch.tensor([[1.0,0,0], [-1,1,1], [-1,-2,4]])\n",
    "\n",
    "X = torch.eig(A, eigenvectors = True)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we worked on a 3x3 square matrix and computed both eigenvalues and eigenvectors. Our respective *real* eigenvalues are 3,2 and 1 as they are the first element in each row of the eigenvalue tensor. The eigenvectors are (0,0,0.57), (0.44,.70,.57), (0.89,.70,.57), note that some of these decimals are repeating in similar fashion meaning if we could represent them as fraction it could look cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 1: A should be square at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/TH/generic/THTensorLapack.cpp:194",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b9824b91bdd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigenvectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 1: A should be square at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/TH/generic/THTensorLapack.cpp:194"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking\n",
    "\n",
    "A = torch.tensor(([1.0,5], [2,4], [0,1]))\n",
    "\n",
    "X = torch.eig(A, eigenvectors=True)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, the eigenvalues and eigenvectors are concepts of linear algebra that apply only to square matrices. Square martices have the same dimensional number of rows and columns, ie(nxn). Here we input a 3x2 matrix and ask PyTorch to compute the eigenvalues and eigenvectors and get an error as should be. Make sure prior to calculations for eigenvectors/eigenvalues that the matrices are square!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary about torch.eig function\n",
    "\n",
    "This function is an important component in lots of machine learning and mathematical code. Finding Eigenvalues and eigenvectors are fundamental problems in Linear Algebra. For square matrices of $ n x n $ dimensions, it seeks to find a non-zero vector x in R^n space such that <b>Ax = &lambda;x</b>. This has applications in mathematical modeling, systems of linear differential equations, rotation of axes problems and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 4 - torch.qr\n",
    "\n",
    "Calculates the QR decomposition of a matrix or batch of matrices. It returns a tuple of (Q,R) of tensors such that the $ input = QR$ where Q is a orthogonal matrix composed of orthonormal columns and R is an upper triangular matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7071, -0.7071,  0.0000],\n",
      "        [-0.7071,  0.7071,  0.0000],\n",
      "        [-0.0000, -0.0000,  1.0000]])\n",
      "tensor([[-1.4142, -2.1213, -0.7071],\n",
      "        [ 0.0000,  0.7071,  0.7071],\n",
      "        [ 0.0000,  0.0000,  2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "A = torch.tensor([[1.0,1,0], [1,2,1], [0,0,2]])\n",
    "\n",
    "q,r = torch.qr(A)\n",
    "\n",
    "print(q)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take a Matrix A of $m$ $x$ $n$ dimensions and rank n and expressed it as the *product* $A=QR$ where Q is an orthogonal matrix and R is an upper triangular matrix. The tensor outputs are in respective order of Q,R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7071, -0.4082, -0.5774],\n",
      "        [-0.0000, -0.8165,  0.5774],\n",
      "        [-0.7071,  0.4082,  0.5774]])\n",
      "tensor([[-1.4142, -0.7071, -2.8284, -8.4853],\n",
      "        [ 0.0000, -1.2247, -2.4495, -7.3485],\n",
      "        [ 0.0000,  0.0000,  1.7321,  1.7321]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "\n",
    "A = torch.tensor([[1.0,1,2,8], [0,1,3,7], [1,0,2,4]])\n",
    "\n",
    "q,r = torch.qr(A)\n",
    "\n",
    "print(q)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the first previous example, here we take another matrix A and computes its orthogonal and upper triangular matrix such that the product $A=QR$ is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"qr_cpu\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-86903e41cc9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"qr_cpu\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking\n",
    "\n",
    "A = torch.tensor([[3,2,1], [9,6,4]])\n",
    "\n",
    "q,r = torch.qr(A, some=False)\n",
    "\n",
    "print(q)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many things that can go wrong with this function since QR decomposition consists of several algorithms and can be performed on any matrix, whether square or rectangular. Therefore I thought it would be good to implement a common error that arises with many functions in PyTorch and is not special to this one particularly. PyTorch expects floating-point tensors as inputs for many of its functions. Here we only used integers and it infered that the data-type is integer but the function work unless we change a number to represent the tensor as float. This can be fixed by adding .0 to any number without changing any information in the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the torch.qr function\n",
    "\n",
    "QR Factorization or decomposition of matrix forms the basis for many algorithms in numerical Linear Algebra. It is involved in many of the computations done in machine learning algorithms. Algorithms for computing eigenvalues, and least squares solutions are based off the concept of QR factorization. Each algorithmic method has its pros and cons and should be evaluated for the best fit to any problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 5 - torch.svd\n",
    "\n",
    "This function computes a singular value decomposition of an input *real* valued matrix or batches of real matrices such that the equation $ input = U*diag(S)*V^T $ is satisfied. Where U is an orthnormal matrix of dimension $m*n$, S is a diagonal matrix of dimension $n*n$, and  V is an $n*n$ dimensional orthonormal matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7071, -0.7071],\n",
      "        [ 0.7071,  0.7071]])\n",
      "tensor([5.0000, 3.0000])\n",
      "tensor([[ 7.0711e-01, -2.3570e-01],\n",
      "        [ 7.0711e-01,  2.3570e-01],\n",
      "        [ 4.2130e-08, -9.4281e-01]])\n"
     ]
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "A = torch.tensor([[3.0,2,2], [2,3,-2]])\n",
    "\n",
    "u, s, v = torch.svd(A)\n",
    "\n",
    "print(u)\n",
    "print(s)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compute the SVD of the 2x3 matrix A. The 3 tensors returned if multiplied from left to right will give us the matrix A. We have decomposed the matrix into a product of 3 matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7071, -0.7071],\n",
      "        [-0.7071,  0.7071]])\n",
      "tensor([3.0000, 1.0000])\n",
      "tensor([[-0.7071,  0.7071],\n",
      "        [-0.7071, -0.7071]])\n"
     ]
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "\n",
    "A = torch.tensor([[1.0,2], [2,1]])\n",
    "\n",
    "u, s, v = torch.svd(A)\n",
    "\n",
    "print(u)\n",
    "print(s)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the first example, we have computed the SVD of the 2x2 matrix A. The 3 tensors represent matrices U, S, V respectively and if multiplied in the order $U*S*V$ will return matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 2 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0056caa1f4c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3 - breaking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msome\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 3 at dim 2 (got 2)"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking\n",
    "\n",
    "A = torch.tensor([[[1,1,0], [0,1,1.0], [0,0,4]], [[1.0,2], [4,3], [5,7]]])\n",
    "\n",
    "u, s, v = torch.svd(A, some=False)\n",
    "\n",
    "print(u)\n",
    "print(s)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we lack consistency in our 2 batches of matrices. The first batch contains a 3x3 matrix and our 2nd batch contains a 3x2 matrix. The input for this function has to be zero or more batch dimensions of tensors that consists of $m * n$ matrices. This can happen during massive datasets and backpropoagation where NA values arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the torch.svd function\n",
    "\n",
    "The Singular Value Decomposition is a powerful mathematical techniques that has many applications. It is a major component of machine learning algorithms and should be understood well. This function is used in backpropoagation algorithms, dimensionality reduction, and many more applications of numerical linear algebra methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this introductory notebook, we went over some basic functions in PyTorch focusing on the Linear Algebra methods, concepts and algorithms. These are fundamental tools in Machine Learning research and applications and are helpful in conducting research or trying to construct new methods. They are used as components in many mathematical applications as well. Determinants and eigenvalues/eigenvectors are characteristics of matrices and help us understand the world of matrix objects by creating new associations and identities. Least Squares, QR decomposition and Singular Value decomposition are powerful applications of linera algebra methods used across a variety of disicplines for approximating solutions and representing matrices as products to be better understood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "Provide links to your references and other interesting articles about tensors\n",
    "* Official documentation for `torch.Tensor`: https://pytorch.org/docs/stable/tensors.html\n",
    "* Elementary Linear Algebra, Fifth Edition, by Larson / Edwards / Falvo\n",
    "* https://en.wikipedia.org/wiki/Singular_value_decomposition\n",
    "* https://en.wikipedia.org/wiki/QR_decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
